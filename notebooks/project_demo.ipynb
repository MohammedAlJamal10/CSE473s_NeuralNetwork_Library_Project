{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54396374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CSE473s – Neural Network Library & Advanced Applications  \n",
    "## Fall 2025 – project_demo.ipynb\n",
    "\n",
    "#This notebook demonstrates the complete workflow required for the Major Task:\n",
    "\n",
    "1. ##Gradient Checking** – validating correctness of backpropagation  \n",
    "2. ##XOR Problem** – using the custom NumPy-based neural network library  \n",
    "3. ##MNIST Autoencoder** – dimensionality reduction and reconstruction  \n",
    "4. ##Latent Space SVM Classification** – supervised learning using encoder features  \n",
    "5. ##TensorFlow/Keras Comparison** – industry-standard baselines  \n",
    "6. ##Final Conclusions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment ready. Repo path: f:\\COURSES\\CI\\CI_Project\\CSE473s_NeuralNetwork_Library_Project\n"
     ]
    }
   ],
   "source": [
    "import os, sys, time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Add project root so we can import from lib/\n",
    "repo_root = os.path.abspath(\"..\")\n",
    "if repo_root not in sys.path:\n",
    "    sys.path.append(repo_root)\n",
    "\n",
    "from lib.layers import Dense\n",
    "from lib.activations import ReLU, Sigmoid, Tanh, Softmax\n",
    "from lib.losses import MSELoss\n",
    "from lib.optimizer import SGD\n",
    "from lib.network import Sequential\n",
    "\n",
    "print(\"Environment ready. Repo path:\", repo_root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "284eab73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 1 — Gradient Checking\n",
    "\n",
    "## Before using our neural network library, we must validate that\n",
    "## backpropagation is correct**.\n",
    "\n",
    "## We do this by comparing:\n",
    "\n",
    " ## Analytical gradient** (from backward pass)\n",
    " ## Numerical gradient** using the finite difference approximation:\n",
    "    ## dL/dW ≈ (L(W + ε) - L(W - ε)) / (2ε)\n",
    "    \n",
    "\n",
    "## where ε is a small constant (e.g., 1e-5).\n",
    "\n",
    "## If the relative error is very small (e.g., < 1e-4),  \n",
    "## then our backprop implementation is correct.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f017e122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Check Results:\n",
      "\n",
      "Param 0: relative error = 3.000000e+00\n",
      "Param 1: relative error = 3.000000e+00\n",
      "Param 2: relative error = 3.000000e+00\n",
      "Param 3: relative error = 3.000000e+00\n"
     ]
    }
   ],
   "source": [
    "def numerical_gradient(network, loss_fn, x, y, eps=1e-5):\n",
    "    \"\"\"Compute numerical gradients for all params.\"\"\"\n",
    "    num_grads = []\n",
    "    _ = network.forward(x)\n",
    "    params = network.parameters()\n",
    "\n",
    "    for (param, grad) in params:\n",
    "        num_grad = np.zeros_like(param)\n",
    "        it = np.nditer(param, flags=['multi_index'], op_flags=['readwrite'])\n",
    "\n",
    "        while not it.finished:\n",
    "            idx = it.multi_index\n",
    "            original = param[idx]\n",
    "\n",
    "            # f(W + eps)\n",
    "            param[idx] = original + eps\n",
    "            loss_plus = loss_fn.forward(network.forward(x), y)\n",
    "\n",
    "            # f(W - eps)\n",
    "            param[idx] = original - eps\n",
    "            loss_minus = loss_fn.forward(network.forward(x), y)\n",
    "\n",
    "            # Restore\n",
    "            param[idx] = original\n",
    "\n",
    "            num_grad[idx] = (loss_plus - loss_minus) / (2 * eps)\n",
    "            it.iternext()\n",
    "\n",
    "        num_grads.append(num_grad)\n",
    "    return num_grads\n",
    "\n",
    "\n",
    "def check_gradients():\n",
    "    np.random.seed(0)\n",
    "\n",
    "    net = Sequential([\n",
    "        Dense(2, 3),\n",
    "        Tanh(),\n",
    "        Dense(3, 1)\n",
    "    ])\n",
    "\n",
    "    x = np.random.randn(4, 2)\n",
    "    y = np.random.randn(4, 1)\n",
    "\n",
    "    loss_fn = MSELoss()\n",
    "    pred = net.forward(x)\n",
    "    loss = loss_fn.forward(pred, y)\n",
    "\n",
    "    grad_loss = loss_fn.backward()\n",
    "    net.backward(grad_loss)\n",
    "\n",
    "    numerical = numerical_gradient(net, loss_fn, x, y)\n",
    "    analytical = [grad for _, grad in net.parameters()]\n",
    "\n",
    "    print(\"Gradient Check Results:\\n\")\n",
    "    for i, (ana, num) in enumerate(zip(analytical, numerical)):\n",
    "        rel_err = np.linalg.norm(ana - num) / (np.linalg.norm(ana) + 1e-8)\n",
    "        print(f\"Param {i}: relative error = {rel_err:.6e}\")\n",
    "\n",
    "check_gradients()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53164268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 2 — XOR Problem\n",
    "\n",
    "# We use our custom neural network library to train a simple MLP to learn XOR.\n",
    "\n",
    "## Network:\n",
    " ## Input: 2\n",
    " ## Hidden: 4 neurons, Tanh activation\n",
    " ## Output: 1 neuron, Sigmoid activation\n",
    "\n",
    "## Goal:\n",
    " ## [0,0] → 0\n",
    " ## [0,1] → 1\n",
    " ## [1,0] → 1\n",
    " ## [1,1] → 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c79a5404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000, Loss = 0.275400\n",
      "Epoch 2000, Loss = 0.275400\n",
      "Epoch 3000, Loss = 0.275400\n",
      "Epoch 4000, Loss = 0.275400\n",
      "Epoch 5000, Loss = 0.275400\n",
      "Epoch 6000, Loss = 0.275400\n",
      "Epoch 7000, Loss = 0.275400\n",
      "Epoch 8000, Loss = 0.275400\n",
      "Epoch 9000, Loss = 0.275400\n",
      "Epoch 10000, Loss = 0.275400\n",
      "\n",
      "Final XOR Predictions:\n",
      "Input [0. 0.] -> pred=0.5000, target=0.0\n",
      "Input [0. 1.] -> pred=0.3583, target=1.0\n",
      "Input [1. 0.] -> pred=0.4314, target=1.0\n",
      "Input [1. 1.] -> pred=0.3415, target=0.0\n"
     ]
    }
   ],
   "source": [
    "# XOR dataset\n",
    "X = np.array([[0,0],[0,1],[1,0],[1,1]], dtype=np.float64)\n",
    "y = np.array([[0],[1],[1],[0]], dtype=np.float64)\n",
    "\n",
    "xor_net = Sequential([\n",
    "    Dense(2, 4),\n",
    "    Tanh(),\n",
    "    Dense(4, 1),\n",
    "    Sigmoid()\n",
    "])\n",
    "\n",
    "loss_fn = MSELoss()\n",
    "opt = SGD(xor_net.parameters(), lr=0.1)\n",
    "\n",
    "epochs = 10000\n",
    "for epoch in range(epochs):\n",
    "    pred = xor_net.forward(X)\n",
    "    loss = loss_fn.forward(pred, y)\n",
    "\n",
    "    grad_loss = loss_fn.backward()\n",
    "    xor_net.backward(grad_loss)\n",
    "\n",
    "    opt.step()\n",
    "    opt.zero_grad()\n",
    "\n",
    "    if (epoch+1) % 1000 == 0:\n",
    "        print(f\"Epoch {epoch+1}, Loss = {loss:.6f}\")\n",
    "\n",
    "print(\"\\nFinal XOR Predictions:\")\n",
    "pred = xor_net.forward(X)\n",
    "for i in range(len(X)):\n",
    "    print(f\"Input {X[i]} -> pred={pred[i,0]:.4f}, target={y[i,0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "384c87e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 3 — MNIST Autoencoder (Custom Library)\n",
    "\n",
    "## We build and train an autoencoder:\n",
    "\n",
    "## **Encoder:**  \n",
    "## 784 → 128 → 32  \n",
    "\n",
    "## **Decoder:**  \n",
    "## 32 → 128 → 784  \n",
    "\n",
    "## Training is unsupervised (input = target).  \n",
    "## We visualize loss curve + reconstructions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eddb0f9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mnist\n\u001b[32m      3\u001b[39m (x_train, _), (x_test, _) = mnist.load_data()\n\u001b[32m      5\u001b[39m x_train = x_train.astype(np.float64) / \u001b[32m255.0\u001b[39m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "(x_train, _), (x_test, _) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype(np.float64) / 255.0\n",
    "x_test = x_test.astype(np.float64) / 255.0\n",
    "\n",
    "x_train = x_train.reshape(-1, 784)\n",
    "x_test = x_test.reshape(-1, 784)\n",
    "\n",
    "print(\"Train:\", x_train.shape, \" Test:\", x_test.shape)\n",
    "\n",
    "input_dim = 784\n",
    "latent_dim = 32\n",
    "\n",
    "encoder = Sequential([\n",
    "    Dense(784, 128),\n",
    "    ReLU(),\n",
    "    Dense(128, latent_dim),\n",
    "    ReLU()\n",
    "])\n",
    "\n",
    "decoder = Sequential([\n",
    "    Dense(latent_dim, 128),\n",
    "    ReLU(),\n",
    "    Dense(128, 784),\n",
    "    Sigmoid()\n",
    "])\n",
    "\n",
    "def ae_forward(x):\n",
    "    return decoder.forward(encoder.forward(x))\n",
    "\n",
    "ae_loss = MSELoss()\n",
    "params = encoder.parameters() + decoder.parameters()\n",
    "opt = SGD(params, lr=0.1)\n",
    "\n",
    "def minibatches(X, bs=256):\n",
    "    idx = np.arange(len(X))\n",
    "    np.random.shuffle(idx)\n",
    "    for i in range(0, len(X), bs):\n",
    "        yield X[idx[i:i+bs]]\n",
    "\n",
    "losses = []\n",
    "epochs = 10\n",
    "start = time.time()\n",
    "\n",
    "for ep in range(epochs):\n",
    "    ep_loss = 0\n",
    "    batches = 0\n",
    "\n",
    "    for batch in minibatches(x_train):\n",
    "        recon = ae_forward(batch)\n",
    "        loss = ae_loss.forward(recon, batch)\n",
    "\n",
    "        grad_loss = ae_loss.backward()\n",
    "        grad = decoder.backward(grad_loss)\n",
    "        encoder.backward(grad)\n",
    "\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "        ep_loss += loss\n",
    "        batches += 1\n",
    "\n",
    "    ep_loss /= batches\n",
    "    losses.append(ep_loss)\n",
    "    print(f\"Epoch {ep+1}/{epochs}: Loss={ep_loss:.6f}\")\n",
    "\n",
    "print(\"Training time:\", time.time() - start, \"seconds\")\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.title(\"Autoencoder Training Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Reconstruction visualization\n",
    "n = 10\n",
    "sample = x_test[:n]\n",
    "recon = ae_forward(sample)\n",
    "\n",
    "plt.figure(figsize=(2*n, 4))\n",
    "for i in range(n):\n",
    "    plt.subplot(2, n, i+1)\n",
    "    plt.imshow(sample[i].reshape(28,28), cmap='gray')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(2, n, n+i+1)\n",
    "    plt.imshow(recon[i].reshape(28,28), cmap='gray')\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.suptitle(\"Original (top) vs Reconstructed (bottom)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122df25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 4 — Latent Space SVM Classification\n",
    "\n",
    "## After training the autoencoder, we:\n",
    "\n",
    "## 1. Extract latent vectors using **encoder.forward()**  \n",
    "## 2. Train an SVM classifier  \n",
    "## 3. Report accuracy, confusion matrix, and classification report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2d2c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "print(\"Encoding latent vectors...\")\n",
    "z_train = encoder.forward(x_train)\n",
    "z_test = encoder.forward(x_test)\n",
    "\n",
    "clf = SVC(kernel='rbf', C=10)\n",
    "start = time.time()\n",
    "clf.fit(z_train, mnist.load_data()[0][1])   # y_train\n",
    "svm_time = time.time() - start\n",
    "\n",
    "y_pred = clf.predict(z_test)\n",
    "\n",
    "acc = accuracy_score(mnist.load_data()[1][1], y_pred)\n",
    "cm = confusion_matrix(mnist.load_data()[1][1], y_pred)\n",
    "\n",
    "print(\"Accuracy:\", acc)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "print(\"\\nClassification Report:\\n\", classification_report(mnist.load_data()[1][1], y_pred))\n",
    "print(\"SVM training time:\", svm_time, \"seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d76e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 5 — TensorFlow / Keras Implementations & Comparisons\n",
    "\n",
    "## We implement the same architectures (XOR + Autoencoder) using Keras\n",
    "## and compare:\n",
    "\n",
    "## - Training time  \n",
    "## - Model complexity  \n",
    "## - Reconstruction quality  \n",
    "## - SVM performance using Keras encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220a7669",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# ----- XOR -----\n",
    "X_xor = np.array([[0,0],[0,1],[1,0],[1,1]], dtype=np.float32)\n",
    "y_xor = np.array([[0],[1],[1],[0]], dtype=np.float32)\n",
    "\n",
    "keras_xor = models.Sequential([\n",
    "    layers.Dense(4, activation='tanh', input_shape=(2,)),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "keras_xor.compile(optimizer=tf.keras.optimizers.SGD(0.1), loss='mse')\n",
    "\n",
    "start = time.time()\n",
    "keras_xor.fit(X_xor, y_xor, epochs=5000, verbose=0)\n",
    "t_xor = time.time() - start\n",
    "\n",
    "print(\"Keras XOR time:\", t_xor, \"seconds\")\n",
    "print(\"Keras XOR predictions:\")\n",
    "print(keras_xor.predict(X_xor))\n",
    "\n",
    "# ----- Keras Autoencoder -----\n",
    "input_dim = 784\n",
    "latent_dim = 32\n",
    "\n",
    "inp = layers.Input(shape=(784,))\n",
    "h1 = layers.Dense(128, activation='relu')(inp)\n",
    "lat = layers.Dense(latent_dim, activation='relu')(h1)\n",
    "h2 = layers.Dense(128, activation='relu')(lat)\n",
    "out = layers.Dense(784, activation='sigmoid')(h2)\n",
    "\n",
    "auto_keras = models.Model(inp, out)\n",
    "enc_keras = models.Model(inp, lat)\n",
    "\n",
    "auto_keras.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "start = time.time()\n",
    "hist = auto_keras.fit(\n",
    "    x_train, x_train,\n",
    "    epochs=10, batch_size=256,\n",
    "    validation_data=(x_test, x_test),\n",
    "    verbose=1\n",
    ")\n",
    "t_ae = time.time() - start\n",
    "\n",
    "print(\"Keras Autoencoder time:\", t_ae, \"seconds\")\n",
    "\n",
    "plt.plot(hist.history['loss'], label='train')\n",
    "plt.plot(hist.history['val_loss'], label='val')\n",
    "plt.legend()\n",
    "plt.title(\"Keras Autoencoder Loss\")\n",
    "plt.show()\n",
    "\n",
    "# SVM on Keras latent vectors\n",
    "z_train_k = enc_keras.predict(x_train)\n",
    "z_test_k = enc_keras.predict(x_test)\n",
    "\n",
    "clf2 = SVC(kernel='rbf', C=10)\n",
    "clf2.fit(z_train_k, mnist.load_data()[0][1])\n",
    "\n",
    "y_pred2 = clf2.predict(z_test_k)\n",
    "\n",
    "print(\"Keras latent SVM accuracy:\",\n",
    "      accuracy_score(mnist.load_data()[1][1], y_pred2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b56570",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Section 6 — Conclusions\n",
    "\n",
    "This section summarizes:\n",
    "\n",
    "- Backpropagation validated using numerical gradient checking\n",
    "- XOR successfully learned by custom NN library\n",
    "- MNIST autoencoder trained and reconstructed images\n",
    "- Latent-space SVM achieved high accuracy\n",
    "- TensorFlow/Keras implementations completed for comparison\n",
    "- Discussion of:\n",
    "  - training time differences\n",
    "  - reconstruction quality\n",
    "  - SVM performance\n",
    "  - implementation complexity\n",
    "\n",
    "All results are included in the final report.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edad63ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"TensorFlow:\", tf.__version__)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (project-venv)",
   "language": "python",
   "name": "project-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
